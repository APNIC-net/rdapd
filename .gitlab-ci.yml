image: alpine:latest

variables:
  AUTO_DEVOPS_DOMAIN: rdapd.tst.apnic.net
  KUBERNETES_VERSION: 1.9.1
  HELM_VERSION: 2.8.2
  CODECLIMATE_VERSION: 0.69.0

stages:
  - package
  - test
  - staging
  - production

.build_test_docker_image:
  stage: package
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - build_and_push_docker_image $TEST_REGISTRY
  only:
    - branches

.unit_tests:
  stage: test
  image: maven:3.5-jdk-8-alpine
  script:
    - setup_test
    - mvn test
  only:
    - branches

.publish_to_test:
  stage: test
  image: $TEST_REGISTRY/ci/alpine-kubectl-helm:1.0.1
  script:
    - publish_chart tst
  only:
      - branches

.publish_to_prod:
  stage: production
  image: $TEST_REGISTRY/ci/alpine-kubectl-helm:1.0.1
  script:
    - publish_chart prod
  when: manual

.code_quality:
  stage: test
  image: docker:stable
  allow_failure: true
  services:
    - docker:stable-dind
  script:
    - setup_docker
    - registry_login
    - code_quality
  artifacts:
    paths: [gl-code-quality-report.json]
  only:
    - branches
  except:
    variables:
      - $CODE_QUALITY_DISABLED

staging:
  stage: staging
  image: $TEST_REGISTRY/ci/alpine-kubectl-helm:1.0.1
  script:
    - configure_cluster
    - initialise_helm
    - ensure_namespace
    - create_secret
    - push_stg_chart
    - deploy_to_stg
  environment:
    name: staging
    #url: http://$CI_PROJECT_PATH_SLUG-staging.$AUTO_DEVOPS_DOMAIN
  only:
    - branches

# ---------------------------------------------------------------------------

.auto_devops: &auto_devops |
  # Auto DevOps variables and functions
  [[ "$TRACE" ]] && set -x
  auto_database_url=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${CI_ENVIRONMENT_SLUG}-postgres:5432/${POSTGRES_DB}
  export DATABASE_URL=${DATABASE_URL-$auto_database_url}
  export CI_APPLICATION_REPOSITORY=$CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG
  export CI_APPLICATION_TAG=$CI_PIPELINE_ID.$CI_COMMIT_SHA
  export CI_CONTAINER_NAME=ci_job_build_${CI_JOB_ID}
  export KUBE_NAMESPACE=${CI_PROJECT_NAME}-${CI_BUILD_REF_SLUG}
  export TILLER_NAMESPACE=$KUBE_NAMESPACE

  function sast_container() {
    docker run -d --name db arminc/clair-db:latest
    docker run -p 6060:6060 --link db:postgres -d --name clair arminc/clair-local-scan:v2.0.1
    apk add -U wget ca-certificates
    docker pull ${CI_APPLICATION_REPOSITORY}:${CI_APPLICATION_TAG}
    wget https://github.com/arminc/clair-scanner/releases/download/v6/clair-scanner_linux_386
    mv clair-scanner_linux_386 clair-scanner
    chmod +x clair-scanner
    touch clair-whitelist.yml
    ./clair-scanner -c http://docker:6060 --ip $(hostname -i) -r gl-sast-container-report.json -l clair.log -w clair-whitelist.yml ${CI_APPLICATION_REPOSITORY}:${CI_APPLICATION_TAG} || true
  }

  function code_quality() {
    docker run --env SOURCE_CODE="$PWD" \
               --volume "$PWD":/code \
               --volume /var/run/docker.sock:/var/run/docker.sock \
               "registry.gitlab.com/gitlab-org/security-products/codequality:$SP_VERSION" /code
  }

  function sast() {
    case "$CI_SERVER_VERSION" in
      *-ee)
        /app/bin/run "$@"
        ;;
      *)
        echo "GitLab EE is required"
        ;;
    esac
  }

  function deploy() {
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    replicas="1"
    service_enabled="false"
    postgres_enabled="$POSTGRES_ENABLED"
    # canary uses stable db
    [[ "$track" == "canary" ]] && postgres_enabled="false"

    env_track=$( echo $track | tr -s  '[:lower:]'  '[:upper:]' )
    env_slug=$( echo ${CI_ENVIRONMENT_SLUG//-/_} | tr -s  '[:lower:]'  '[:upper:]' )

    if [[ "$track" == "stable" ]]; then
      # for stable track get number of replicas from `PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_slug}_REPLICAS
      service_enabled="true"
    else
      # for all tracks get number of replicas from `CANARY_PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_track}_${env_slug}_REPLICAS
    fi
    if [[ -n "$new_replicas" ]]; then
      replicas="$new_replicas"
    fi

    helm upgrade --install \
      --force \
      --wait \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      $(test_dep_name $name) \
      helm/test/

    helm upgrade --install \
      --force \
      --wait \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set application.track="$track" \
      --set application.database_url="$DATABASE_URL" \
      --set service.url="$CI_ENVIRONMENT_URL" \
      --set replicaCount="$replicas" \
      --set postgresql.enabled="$postgres_enabled" \
      --set postgresql.nameOverride="postgres" \
      --set postgresql.postgresUser="$POSTGRES_USER" \
      --set postgresql.postgresPassword="$POSTGRES_PASSWORD" \
      --set postgresql.postgresDatabase="$POSTGRES_DB" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      --values=helm/test-env-values.yaml \
      "$name" \
      helm/rdapd/
  }

  function configure_cluster() {
    echo "Configuring cluster"
    echo $TEST_CLUSTER_CA_BUNDLE > test_cluster_ca.crt
    cat test_cluster_ca.crt
    kubectl config set-cluster --kubeconfig=ci ci --server=$TEST_CLUSTER_SERVER --certificate-authority=test_cluster_ca.crt --insecure-skip-tls-verify=true
    #kubectl config set-cluster --kubeconfig=ci ci --server=$TEST_CLUSTER_SERVER --certificate-authority=test_cluster_ca.crt --insecure-skip-tls-verify=true
    kubectl config set-context  --kubeconfig=ci ci --namespace=${KUBE_NAMESPACE} --cluster=ci --user=ci
    kubectl config set-credentials --kubeconfig=ci ci --token=$KUBE_TOKEN
    kubectl config use-context --kubeconfig=ci ci
    export  KUBECONFIG=ci
    echo $KUBE_NAMESPACE
  }

  function push_stg_chart() {
    # Using yq until helm package supports --set
    # https://github.com/kubernetes/helm/issues/3141
    yq write -i helm/rdapd/values.yaml image.repository "$CI_APPLICATION_REPOSITORY"
    yq write -i helm/rdapd/values.yaml image.tag "$CI_APPLICATION_TAG"
    # Workaround for package requiring chart and folder names to match
    # https://github.com/kubernetes/helm/issues/1979
    export CHART_NAME="$(yq read helm/rdapd/Chart.yaml name)"
    ln -sf chart "helm/$CHART_NAME"
    helm package --version $CI_BUILD_ID "helm/$CHART_NAME"
    export AWS_ACCESS_KEY_ID=$CI_CHART_REPO_USER
    export AWS_SECRET_ACCESS_KEY=$CI_CHART_REPO_TOKEN
    export AWS_DEFAULT_REGION=us-east-1
    export AWS_ENDPOINT=10.2.1.221:9000
    export AWS_DISABLE_SSL=true
    helm repo add staging s3://staging-charts/
    helm s3 push "./$CHART_NAME-$CI_BUILD_ID.tgz" staging
  }

  function deploy_to_stg() {
    helm upgrade --install \
      --force \
      --wait \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_BUILD_ID" \
      --values=helm/staging-env-values.yaml \
      "$CHART_NAME" \
      "staging/$CHART_NAME"
  }

  function test_dep_name() {
    local name=$1
    echo "$name-test-env"
  }

  function install_dependencies() {
    apk add -U openssl curl tar gzip bash ca-certificates git
    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-glibc/master/sgerrand.rsa.pub
    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.23-r3/glibc-2.23-r3.apk
    apk add glibc-2.23-r3.apk
    rm glibc-2.23-r3.apk

    curl "https://kubernetes-helm.storage.googleapis.com/helm-v${HELM_VERSION}-linux-amd64.tar.gz" | tar zx
    mv linux-amd64/helm /usr/bin/
    helm version --client

    curl -L -o /usr/bin/kubectl "https://storage.googleapis.com/kubernetes-release/release/v${KUBERNETES_VERSION}/bin/linux/amd64/kubectl"
    chmod +x /usr/bin/kubectl
    kubectl version --client
  }

  function setup_docker() {
    if ! docker info &>/dev/null; then
      if [ -z "$DOCKER_HOST" -a "$KUBERNETES_PORT" ]; then
        export DOCKER_HOST='tcp://localhost:2375'
      fi
    fi
    mkdir -p /root/ssl/certs/
    cp registry.crt /etc/ssl/certs/$CI_REGISTRY.crt
    cp registry.crt /usr/local/share/ca-certificates/$CI_REGISTRY.crt
    update-ca-certificates

    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" "$CI_REGISTRY"
  }

  function setup_test() {
    export MAVEN_MIRROR_OF=$MAVEN_MIRROR_OF
    export MAVEN_MIRROR_URL=$MAVEN_MIRROR_URL
    mkdir /root/.m2/
    cp maven_settings.xml /root/.m2/settings.xml
  }

  function initialise_helm() {
    chart_dir=helm/rdapd
    helm init --client-only --skip-refresh
    helm dependency update "$chart_dir/"
    helm dependency build "$chart_dir/"
  }

  function ensure_namespace() {
    kubectl config view
    kubectl describe namespace "$KUBE_NAMESPACE" || kubectl create namespace "$KUBE_NAMESPACE"
  }

  #Takes one argument: the repository server (test or prod)
  function build_and_push_docker_image() {
    echo "Creating and pusing docker image"
    echo "{\"auths\":{\"$1\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}" > /kaniko/.docker/config.json
    cat registry.crt  >> /kaniko/ssl/certs/ca-certificates.crt
    echo "$1/$REGISTRY_PATH/$CI_COMMIT_REF_SLUG:$CI_APPLICATION_TAG"
    /kaniko/executor --context $CI_PROJECT_DIR --build-arg MAVEN_MIRROR_OF=$MAVEN_MIRROR_OF --build-arg MAVEN_MIRROR_URL=$MAVEN_MIRROR_URL --build-arg HTTP_PROXY=$HTTP_PROXY --build-arg HTTPS_PROXY=$HTTPS_PROXY --build-arg NO_PROXY=$NO_PROXY --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $1/$REGISTRY_PATH/$CI_COMMIT_REF_SLUG:$CI_APPLICATION_TAG
  }

  function publish_chart() {
    mkdir /root/helm_package/
    helm package -d /root/helm_package/ helm/rdapd
    cd /root/helm_package
    echo "curl -s --data-binary \"@$(ls -A1 /root/helm_package/)\" $CHART_API_URI/api/$1/charts"
    curl -s --data-binary "@$(ls -A1 /root/helm_package/)" $CHART_API_URI/api/$1/charts
  }


before_script:
  - *auto_devops
